# Cybercrime and computer forensics

Notes by Nino Filiu; based on the Forensics course by David Balzarotti.

| Slides file | completion status |
| --- | --- |
| [intro](https://my.eurecom.fr/upload/docs/application/pdf/2018-03/malware_intro.pdf) | done |
| [static analysis A](https://my.eurecom.fr/upload/docs/application/pdf/2018-03/static_analysis_a_2018-03-22_10-54-54_196.pdf) | done |
| [static analysis B](https://my.eurecom.fr/upload/docs/application/pdf/2018-03/static_analysis_b.pdf) | done |
| [reverse engineering tools](https://my.eurecom.fr/upload/docs/application/pdf/2018-04/reveng_tools.pdf) | ignored |
| [dynamic analysis A](https://my.eurecom.fr/upload/docs/application/pdf/2018-04/malware_analysis_sandboxes.pdf) | in progress |
| [dynamic analysis B](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/dynamic_analysis_part_b.pdf) | to do |
| [computer forensics](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/forensics_intro_wide.pdf) | to do |
| [memory forensics](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/memory_forensics.pdf) | to do |
| [network forensics A](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/network_forensics_first_half.pdf) | to do |
| [network forensics B](https://my.eurecom.fr/upload/docs/application/pdf/2018-06/network_forensics.pdf) | to do |
| [forensics techniques](https://my.eurecom.fr/upload/docs/application/pdf/2018-06/basic_techniques.pdf) | to do |
| [logical reasonning A](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/logical_reasoning_1.pdf) | to do |
| [logical reasonning B](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/logical_reasoning_2.pdf) | to do |

## Binary and malware analysis

### Glossary

A **malware** is a piece of software intentionally designed with malicious functionalities. SQL injections and the likes are not malware because they are not software. Vulnerable applications are not malwares because the faults are not intentional. Malicious = against the user's will. These 3 concepts are blurry so the frontier between what's a malware and what's not is blurry too.

There are tons of types of malware: **viruses, worms, trojan, ransomware, botnets, adware, cryptominers**... The course is not aiming at covering them all, rather studying some of them when they can be used as examples.

**Binary** can denote program binary and data binary. Program binary is usually generated by a compiler... but not always! The world of well-formed binaries that use functions, stacks, heaps and the likes is theoretically a grain of sand in the Sahara of possible executables. Binaries mostly come in these forms though: **executable files, libraries, firmware images, and process dumps**.

Malicious binaries are often very **adversarial**: their symbols are stripped of, they are obsfuscated, packed, and full of tricks (anti-debugging, suicide bombs, checks for sandbox...).

### History

**80's: the origins**: first widespread outbreak of a virus. Leonard Adleman coins the term virus. first antivirus company (McAfee). First worm.

**90's: complexity grows**: the term malware is coined. Polymorphic viruses. Virus that damages AV. Melissa. Kernel rootkits.

**00's: the new millenium**: code red. SQL slammer. Spam botnets.

**05's: cybercrime**: Storm (worm+botnet). Zeus banking trojan. Koobface. Microsoft offers $250K who developed Conficker, the first pandemic virus.

**10's: state-sponsored**: Stuxnet sabotages iranian nuclear facilities. Gauss. leak of the NSA ANT catalog. the Mirai botnet is responsible for a large DDoS attack using infected IoT. increase in mobile infection.

### Malware analysis

The goal is to find out: 

* what the malware can do
* what the malware actually does
* which family does it belongs to
* how can it be detected
* how can it be blocked
* how can it be removed

Part of the larger field that is reverse engineering. Reverse engineering has many legitimate applications but it is not legal everywhere in the same form. Due to the large amount of pieces of code to analyze, automated binary analysis is often used: there is however a trade-off between scalability and precision+unbiasement. Several stages to detect a malware:

```
def detect(prog):
    if prog.isKnown:
        # often fail because of polymorphism
        return true
    if prog.matchesAKnownSignature:
        # signatures are often imprecise
        return true
    if staticAnalysis(prog):
        # malware code is highly obfuscated, encrypted, self-modified...
        return true
    if dynamicAnalysis(prog):
        # many expensive parameters
        # limited time
        # can behave badly only at certain times
        # sandbox detection
        return true
    return false
```

So malware detection is hard. Anlasysis are done in an adversial environment so the data is noisy and ML performs badly, plus there is a demand for a zero tolerance on false positive. Complex problems that range from microscopic programming (flipping bits) to macroscopic programming (intelligence from billions of aggregated infos).

This have to be done for every sample, and there are approx 1M samples/day.

Dynamic analysis are more precise because it tells you everything the program can do. However, it achieves a smaller coverage, given that it observes one execution path at a time. Static analysis, on the contrary, are less precise because it is needed to reason about the program behavior without executing it, but it achieves a larger coverage because it can reason about all possible executions at the same time.



## Static analysis

### Black box

Overview:

* known binary: check for the file hash
* similar to something known: check the signatures
* looking for hints: study embedded strings, imported libs, file headers, and symbols

Diving into it requires some knowledge about available tools and additional knowledge about binaries.

**VirusTotal** is a service that can help with file hashes - it knows more than 1B of files. **Yara** is a language to describe byte-level patterns, and a tool to match the patterns against files - kind of a grep on steroids. Command-line or Python-API usable. **strings** is a command-line utility that locates the printable strings of characters in a file. **Windows's dependency-walker and Linux' ldd/lddtree** check for dynamic libraries linking. Analysis is much harder for statically linked ones - their binary is not really differentiable from the program's binary.

Binary file format defines what the file looks like on disk and how it should be loaded in memory. **PE (portable executable)** is used to represent executables in windows, **ELF (executable and linkable format)** is the equivalent in UNIX-like OS.

#### Portable executable format

Contains multiple section of either code or data, each of them having its particular RWX attributes shared by all processes running the executable. Compilers have a standard set of sections but programmers are free to create and name arbitrary ones. Memory addresses are expressed using RVA (relative virtual addresses). A Data Directory array is used to locate other artifacts and data structures inside the PE. Each imported DLL has a structure in the PE, containing the name of the DLL and an array of function pointers known as the Import Address Table (IAT). Structure:

```
MS-DOS
    header
    stub
PE header
    file header
    optional header
sections table
    section 1 header
    ...
    section N header
section 1
...
section N
```

Explanation:

* MS-DOS: small executable that checks for windows
* PE file header: basic infos: 32 or 64 bits, nb of sections...
* PE optional header: optional infos: size of code, entry point address, size of headers...
* common sections (not extensive):
	* .text: default code
	* .data: default RW data section (global variables)
	* .rdata: default R data section
	* .idata: imports table, one entry per imported library.

**HT** is a utility for editing and visualizing the executable sections. **pefile** is a python lib to analyze PE files, thus the name pefile lol. Suspicious PE attributes can be used to flag potentially malicious binaries: weird entry points, high entropy = packers... **pescanner.py** can be used to locate such things, PEStudio too.

#### Executable and linkable format

```
ELF header
    // file type, machine arch, magic number, code entry point...
Program header table
    // type, offset, vaddr and paddr...
Section 1
...
Section N
    // .bss, .data, .rodata, .text...
Section header table
    // for each section, a name, a type, flags, address, offset, and the likes are precised
```

Since the loader and dynamic linker only reason in terms of segments, section information is not required at runtime. One can get rid of this section with `truncate -s $(readelf -h file.elf | grep -F 'Start of section headers' | awk '{print $5}') file.elf.`

The symbol table holds quite precious (on a human point of view) infos: name/value/size/section of each symbol, global/local/weak binding of them... Programs still run with this table removed but all the names of the program functions and global variables are lost. In the case of statically-linked programs, hundreds of nameless library functions are mixed with the program code.

Useful functions:

```
$ readelf <options> <filename>
    - print headers
    - print the symbols
    - print the notes
$ ldd <progname>
$ lddtree <progname>
    - list the shared libs required by the prog
$ truncate -s $(readelf -h file.elf | grep -F 'Start of section headers' | awk '{print $5}') file.elf.
		The loader and linker only reason in terms of segments so section info is not required at runtime.
		This command strips it out.
```

### Assembly 101

Two main families of instruction sets: RISC and CISC. An assembler does the machine code - assembly translation, assembly being the human readable and (almost) instruction-to-instruction equivalent of the machine code. Let's take a closer look at the x86 CISC assembler.

* Each instruction is in the form mnemonic arguments
* 0 to 2 arguments
* constant, registers, or mem addresses arguments
* variable length
* two main syntax: intel (used in this course) and AT&T (default).

Registers can be accessed like so:

```
RAX [ 0:64] bits
EAX [32:64] bits
AH  [32:48] bits
AL  [48:64] bits
```

In x86, instructions fall in several categories:

* data movement: mov, push, pop, lea...
* arithmetic and logic: add, sub, mul, not, and, or...
* control flow: jmp, call, ret...

There are many calling conventions, the one used in C (CDEL) is the standard. Arguments are passed on the sack right to left, returnn value is placed in the EAX, the calling function cleans the stack, and EAX/ECX/EDX are free to use. However, there are many calling conventions: STDCALL→called function cleans the stack, FASTCALL→first params passed in registers...

An ABI (application binary interface) defines the interface between software modules or userspace to kernel communication: calling convention, data types...

### Disassembly

Assembly is deterministic but disassembly is not, ie from a sequence of bytes it is theoretically undecideable to deduce a single assembly code with absolute certainty (surprising, eh?). Main reasons:

* depending from which byte the disassembler starts, different instructions can be deduced
* data and code are mixed and it's not always feasible to know which is which

Two main approaches to disassemble a program: linear sweep = instr per instr, recursive traversal = control flow is followed. Linear sweep is simpler but assumes a well-behaved compiler that packs every instruction together: objdump, gdb, windbg use this approach. In recursive traversal, it's hard to know which branch to disassemble without actually running the program, and that's not even taking into account overlapping functions self-modifying code.

But disassemblers have tricks to counter these difficulties: functions are often detected easily thanks to their prologue (usual sequences of code before a function).

### Decompilation

dissassembly → code. Several limitations:

* requires a perfect disassembly
* compilation is lossy
* complex data structure makes the code hard to read
* compilation = many-to-many operation, theoretically impossible to reverse with certainty
* language and computer-specific
* expensive lol (hexrays = 2600 buck)

Usually follow this pattern:

1. disassembly
2. dataflow analysis: recognize variables, detach them from registers and memory addresses; identify function args
3. control flow analysis:
4. type analysis

### Limits of static analysis

In a nutshell:

* packing
* indirect jump prediction
* obfuscated addresses
* limits of disassembly:
	* overlapping instructions
	* fake conditional jumps
	* return address modification

### Packers

`code ---(compression and/or encryption)--> unpacking stub + data`

The unpacking stub retrieves the original code from the data. The data is often stored in weird PE sections. It also resolves the imports. It then `jump/call/ret` to the OEP (original entry point). Source of a lot of complication for the analysis:

* **Recursive layers**
* **Interleaved packing**: no clear jump tail, but the code is interleaved with the code of the original app
* **Partial unpacking**: unpack only one function or block at a time. Sometimes re-pack it so that the code is never fully unpacked (=statically analisable)
* **Emulation**: translate the instructions into a random bytecode. The stub includes an interpreter

There are many packers but their presence can often be detected by:

* Few functions
* Few entries in the import table
* Small code but requires more space in the memory disk
* Sections with weird names
* Sections with high entropy

Tools like SigBuster and PeiD can be used to identify packers.

The many packers that exist can be grouped into types:

1. single-layer
2. multi-layer, linear transitions
3. cyclic transition, tail isolation
4. interleaved isolation, single frame
5. multi-frame, incremental frame switch
6. shifting decode frames

A few % of packers are of types 5 and above, but less than 30 of types 2 and below. Most packers aare of type 3.

There also exists several types of unpacking techniques:

* **Automatic + static**: very few cases that are not security-oriented
* **Automatic + dynamic**: euristics to detect the OEP. Works well for simple packers
* **Manual + static**: requires a reversing of the stub
* **Manual + dynamic**: use a debugger to manually id the OEP

## Reverse engineering tools

Putting this part into notes seems useless - here are [the original slides](https://my.eurecom.fr/upload/docs/application/pdf/2018-04/reveng_tools.pdf)

## Dynamic analysis

Tells you exactly what the program does in a given environment and with a particular input. Therefore, one must collect env infos before performing such an analysis (ex: did it spread through malicious PDFs? In which OS was it detected?).

The idea of a sandbox is crucial. The idea is to run the malware inside an isolated and instrumented environment in order to observe the program's behavior. It makes the analysis immune to static obfuscation, anti-disas tricks, and packers. The sandbox must be able to:

* be easily reverted to a pristine state **(setup)**
* control and contain the malicious activities **(containment)**
* collect infos about the running sample **(instrumentation)**

### Chosing the right sandbox analysis method

**automated analysis / manual analysis**: automated is fast and the configuration is already supported by a team of experts, but manual analysis lets one tweak the inputs, and create an environment not known by attackers.